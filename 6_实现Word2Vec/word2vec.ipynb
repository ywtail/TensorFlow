{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "使用 urllib.urlretrieve 下载数据的压缩文件，并核对尺寸，如果已经下载了文件则跳过。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Found and verified', 'text8.zip')\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urllib.urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception('Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "在浏览器地址栏输入 http://mattmahoney.net/dc/text8.zip 下载数据的压缩文件。\n",
    "\n",
    "接下来解压下载的压缩文件，并使用 tf.compat.as_str 将数据转成单词的列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 17005207\n"
     ]
    }
   ],
   "source": [
    "# 将词存入 word 列表中\n",
    "def read_data(filename):\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data\n",
    "\n",
    "words = read_data(filename)\n",
    "print 'Data size', len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "通过输出知道数据最后被转为了一个包含 17005207 个单词的列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 50000  # 将出现频率最高的 50000 个单词放入 count 列表中，然后放入 dicionary 中\n",
    "\n",
    "\n",
    "def build_dataset(words):\n",
    "    count = [['UNK', -1]]  # 前面是词汇，后面是出现的次数，这里的 -1 在下面会填上 UNK 出现的频数\n",
    "    # 将出现频次最高的 50000 个词存入count\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))  # -1 因为 UNK 已经占了一个了\n",
    "\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    '''\n",
    "    等价于，就是按 count 中词出现的顺序，分别给他们编号：0 1 2 ...\n",
    "        for i in vocabulary_size:\n",
    "            dictionary[count[i][0]]=i\n",
    "    '''\n",
    "    # 编码：如果不出现在 dictionary 中，就以 0 作为编号，否则以 dictionary 中的编号编号   \n",
    "    # 也就是将 words 中的所有词的编号存在 data 中，顺带查一下 UNK 有多少个，以便替换 count 中的 -1\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "\n",
    "    count[0][1] = unk_count\n",
    "\n",
    "    # 编号：词\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "words[:10]\n",
    "\n",
    "输出：\n",
    "['anarchism',\n",
    " 'originated',\n",
    " 'as',\n",
    " 'a',\n",
    " 'term',\n",
    " 'of',\n",
    " 'abuse',\n",
    " 'first',\n",
    " 'used',\n",
    " 'against']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "data[:10] \n",
    "\n",
    "输出：\n",
    "[5239, 3084, 12, 6, 195, 2, 3137, 46, 59, 156]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "count[:10]\n",
    "\n",
    "输出：\n",
    "[['UNK', 418391],\n",
    " ('the', 1061396),\n",
    " ('of', 593677),\n",
    " ('and', 416629),\n",
    " ('one', 411764),\n",
    " ('in', 372201),\n",
    " ('a', 325873),\n",
    " ('to', 316376),\n",
    " ('zero', 264975),\n",
    " ('nine', 250430)]\n",
    "\n",
    "dictionary # 词：编号\n",
    "\n",
    "输出：\n",
    "{'fawn': 45848,\n",
    " 'homomorphism': 9648,\n",
    " 'nordisk': 39343,\n",
    " 'nunnery': 36075,\n",
    " 'chthonic': 33554,\n",
    " 'sowell': 40562,\n",
    " 'sonja': 38175,\n",
    " 'showa': 32906,\n",
    " 'woods': 6263,\n",
    " 'hsv': 44222,\n",
    " 'spiders': 14623,\n",
    " 'hanging': 8021,\n",
    " 'woody': 11150,\n",
    " ...\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "dictionary['UNK']\n",
    "\n",
    "输出：\n",
    "0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "dictionary['a']\n",
    "\n",
    "输出：\n",
    "6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "reverse_dictionary # 编号：词\n",
    "\n",
    "输出：\n",
    "{0: 'UNK',\n",
    " 1: 'the',\n",
    " 2: 'of',\n",
    " 3: 'and',\n",
    " 4: 'one',\n",
    " 5: 'in',\n",
    " 6: 'a',\n",
    " 7: 'to',\n",
    " 8: 'zero',\n",
    " ...\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common word (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
      "Sample data [5239, 3084, 12, 6, 195, 2, 3137, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
     ]
    }
   ],
   "source": [
    "del words  # 删除原始单词表，节约内存\n",
    "print 'Most common word (+UNK)', count[:5]  # 打印最高频词汇及其出现次数（包括Unknow词汇）\n",
    "print 'Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]]  # 前10个单词编码、单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 生成 Word2Vec 训练样本\n",
    "data_index = 0\n",
    "\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index  # 设为global 因为会反复 generate\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "\n",
    "    # 将 batch 和 labels 初始化为数组\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "\n",
    "    # 对某个单词创建相关样本时会使用到的单词数量，包括目标单词本身和它前后的单词\n",
    "    span = 2 * skip_window + 1\n",
    "\n",
    "    # 创建最大容量为 span 的 deque（双向队列）\n",
    "    # 在用 append 对 deque 添加变量时，只会保留最后插入的 span 个变量\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "\n",
    "    # 从 data_index 开始，把 span 个单词顺序读入 buffer 作为初始值，buffer 中存的是词的编号\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    # buffer 容量是 span，所以此时 buffer 已经填满，后续的数据将替换掉前面的数据\n",
    "\n",
    "    # 每次循环内对一个目标单词生成样本，前方已经断言能整除，这里使用 // 是为了保证结果是 int\n",
    "    for i in range(batch_size // num_skips):  # //除法只保留结果整数部分（python3中），python2中直接 /\n",
    "        # 现在 buffer 中是目标单词和所有相关单词\n",
    "        target = skip_window  # buffer 中第 skip_window 个单词为目标单词（注意第一个目标单词是 buffer[skip_window]，并不是 buffer[0]）\n",
    "        targets_to_avoid = [skip_window]  # 接下来生成相关（上下文语境）单词，应将目标单词拒绝\n",
    "\n",
    "        # 每次循环对一个语境单词生成样本\n",
    "        for j in range(num_skips):\n",
    "            # 先产生一个随机数，直到随机数不在 targets_to_avoid 中，就可以将之作为语境单词\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)  # 因为这个语境单词被使用了，所以要加入到 targets_to_avoid\n",
    "\n",
    "            batch[i * num_skips + j] = buffer[skip_window]  # feature 是目标词汇\n",
    "            labels[i * num_skips + j, 0] = buffer[target]  # label 是 buffer[target]\n",
    "\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "调用 generate_batch 函数测试一下功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3084 originated -> 12 as\n",
      "3084 originated -> 5239 anarchism\n",
      "12 as -> 6 a\n",
      "12 as -> 3084 originated\n",
      "6 a -> 195 term\n",
      "6 a -> 12 as\n",
      "195 term -> 2 of\n",
      "195 term -> 6 a\n"
     ]
    }
   ],
   "source": [
    "batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)\n",
    "for i in range(8):\n",
    "    print batch[i], reverse_dictionary[batch[i]], '->', labels[i, 0], reverse_dictionary[labels[i, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 训练需要的参数\n",
    "batch_size = 128\n",
    "embedding_size = 128  # 将单词转为稠密向量的维度，一般是500~1000这个范围内的值，这里设为128\n",
    "skip_window = 1  # 单词间最远可以联系到的距离\n",
    "num_skips = 2  # 对每个目标单词提取的样本数\n",
    "\n",
    "# 生成验证数据，随机抽取一些频数最高的单词，看向量空间上跟它们距离最近的单词是否相关性比较高\n",
    "valid_size = 16  # 抽取的验证单词数\n",
    "valid_window = 100  # 验证单词只从频数最高的 100 个单词中抽取\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)  # 随机抽取\n",
    "num_sampled = 64  # 训练时用来做负样本的噪声单词的数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # 建立输入占位符\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)  # 将前面随机产生的 valid_examples 转为 TensorFlow 中的 constant\n",
    "\n",
    "    with tf.device('/cpu:0'):  # 限定所有计算在 CPU 上执行\n",
    "        # 随机生成所有单词的词向量 embeddings，单词表大小 5000，向量维度 128\n",
    "        embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "        # 查找 train_inputs 对应的向量 embed\n",
    "        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "        # 使用 NCE Loss 作为训练的优化目标\n",
    "        nce_weights = tf.Variable(\n",
    "            tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        nce_bias = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # 使用 tf.nn.nce_loss 计算学习出的词向量 embed 在训练数据上的 loss，并使用 tf.reduce_mean 进行汇总   \n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(weights=nce_weights, biases=nce_bias, labels=train_labels, inputs=embed, num_sampled=num_sampled,\n",
    "                       num_classes=vocabulary_size))\n",
    "\n",
    "    # 定义优化器为 SGD，且学习速率为 1.0\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "    # 计算嵌入向量 embeddings 的 L2 范数 norm\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    # 标准化\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    # 查询验证单词的嵌入向量，并计算验证单词的嵌入向量与词汇表中所有单词的相似性\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "    # 初始化所有模型参数\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0 : 253.554870605\n",
      "Nearest to use : flyers, unborn, comprehended, sega, eliminating, prophetic, nano, empties,\n",
      "Nearest to its : commandments, natura, eon, bed, timur, molyneux, evangelicalism, chaco,\n",
      "Nearest to seven : immortalized, wenham, unclaimed, typewriters, af, counterbalanced, mansion, roommates,\n",
      "Nearest to there : testosterone, majdanek, usage, numerology, sulphur, triad, bad, cherry,\n",
      "Nearest to by : unsigned, salinger, sitter, karaca, opel, undeniably, portugal, et,\n",
      "Nearest to to : amp, snowflakes, melatonin, sideways, quarrels, milhaud, isla, tonle,\n",
      "Nearest to all : ontology, eraserhead, griffey, curses, listens, howstuffworks, polar, mexicana,\n",
      "Nearest to first : blow, rangoon, volution, crimp, compostela, manchuria, subspecies, sixteen,\n",
      "Nearest to history : shaman, alvar, russians, blaxploitation, numeration, halfdan, duckling, wondered,\n",
      "Nearest to at : kbe, telegraphic, cats, uri, generational, baseline, regression, maoists,\n",
      "Nearest to which : alvarez, magee, dios, sojourn, rabbani, xenon, inductively, flask,\n",
      "Nearest to see : cubes, considers, heero, upto, germ, ah, tattoo, whipped,\n",
      "Nearest to are : maois, chute, bionic, kept, unguided, exacted, panth, mieszko,\n",
      "Nearest to five : arithmetica, blocs, conflicted, miracle, expounded, disband, hakim, derwent,\n",
      "Nearest to often : ethnic, ltte, adria, fortieth, cad, mend, tuileries, involvement,\n",
      "Nearest to may : waza, pashtun, oxygenated, ulcers, tragedians, rehearsals, outgoing, edom,\n",
      "Average loss at step 2000 : 113.060384287\n",
      "Average loss at step 4000 : 53.0690288844\n",
      "Average loss at step 6000 : 33.2609856217\n",
      "Average loss at step 8000 : 23.0952055303\n",
      "Average loss at step 10000 : 18.2373447652\n",
      "Nearest to use : vs, victoriae, baseball, vector, opiate, alpina, injured, reginae,\n",
      "Nearest to its : commandments, the, bed, reginae, akira, decades, victoriae, a,\n",
      "Nearest to seven : nine, zero, cl, vs, mathbf, reginae, explicitly, analogue,\n",
      "Nearest to there : bad, gland, potato, usage, renovate, november, it, structures,\n",
      "Nearest to by : and, in, as, austin, UNK, was, nine, are,\n",
      "Nearest to to : and, with, in, for, austin, vs, cl, one,\n",
      "Nearest to all : gland, implicit, polar, ontology, order, ancient, grapheme, cat,\n",
      "Nearest to first : infections, vs, latitude, blow, best, public, mcclellan, one,\n",
      "Nearest to history : russians, alpina, saul, kurtz, victoriae, bce, tickets, pervasive,\n",
      "Nearest to at : cl, two, on, bucks, austin, of, kbe, presumably,\n",
      "Nearest to which : helicopters, tuberculosis, flask, comparative, never, agricultural, litigation, burgeoning,\n",
      "Nearest to see : canaris, centuries, gland, considers, ah, fan, blend, equipped,\n",
      "Nearest to are : is, by, and, bionic, constellations, gland, kept, were,\n",
      "Nearest to five : reginae, alpina, victoriae, nine, gland, three, zero, cl,\n",
      "Nearest to often : ethnic, accordion, balance, published, dreyfus, bang, austin, alchemy,\n",
      "Nearest to may : waza, vs, gogh, hannah, substantially, assesses, pashtun, bonding,\n",
      "Average loss at step 12000 : 14.3100124947\n",
      "Average loss at step 14000 : 11.8394078088\n",
      "Average loss at step 16000 : 9.94490135089\n",
      "Average loss at step 18000 : 8.49552821398\n",
      "Average loss at step 20000 : 8.02561856115\n",
      "Nearest to use : unborn, velar, victoriae, vs, dasyprocta, opiate, vector, prophetic,\n",
      "Nearest to its : the, his, their, bed, metis, decades, commandments, moravia,\n",
      "Nearest to seven : nine, zero, five, eight, two, agouti, four, three,\n",
      "Nearest to there : it, backus, he, gland, dasyprocta, bad, potato, they,\n",
      "Nearest to by : in, was, as, with, from, for, is, dasyprocta,\n",
      "Nearest to to : for, would, and, vs, valentinians, dasyprocta, agouti, in,\n",
      "Nearest to all : subkey, gland, order, dasyprocta, ontology, polar, journalists, ancient,\n",
      "Nearest to first : rangoon, best, brady, agouti, amman, public, blow, vs,\n",
      "Nearest to history : alvar, saul, russians, tickets, alphorn, alpina, kurtz, pervasive,\n",
      "Nearest to at : in, on, agouti, cl, and, of, bodybuilding, two,\n",
      "Nearest to which : that, never, also, and, dasyprocta, metis, it, comparative,\n",
      "Nearest to see : canaris, is, considers, ah, fan, blend, centuries, dictionaries,\n",
      "Nearest to are : is, were, was, by, in, gland, mg, asher,\n",
      "Nearest to five : zero, eight, nine, seven, three, six, two, dasyprocta,\n",
      "Nearest to often : mattingly, dasyprocta, now, accordion, ethnic, mats, balance, io,\n",
      "Nearest to may : waza, tragedians, dasyprocta, eight, three, gogh, nur, would,\n",
      "Average loss at step 22000 : 7.2253218497\n",
      "Average loss at step 24000 : 6.99454965758\n",
      "Average loss at step 26000 : 6.6251022315\n",
      "Average loss at step 28000 : 6.20031205404\n",
      "Average loss at step 30000 : 6.14926322377\n",
      "Nearest to use : unborn, velar, victoriae, dasyprocta, vs, prophetic, azimuth, opiate,\n",
      "Nearest to its : the, their, his, a, bed, moravia, decades, metis,\n",
      "Nearest to seven : eight, five, nine, four, six, three, zero, two,\n",
      "Nearest to there : it, they, he, still, dasyprocta, she, gland, potato,\n",
      "Nearest to by : was, in, as, with, from, for, and, is,\n",
      "Nearest to to : for, would, in, from, can, eight, nine, and,\n",
      "Nearest to all : subkey, gland, abitibi, dasyprocta, ontology, journalists, order, polar,\n",
      "Nearest to first : best, rangoon, agouti, brady, amman, infections, vs, blow,\n",
      "Nearest to history : shaman, saul, tickets, alvar, russians, alphorn, alpina, kurtz,\n",
      "Nearest to at : in, on, and, agouti, with, for, cl, dasyprocta,\n",
      "Nearest to which : that, also, it, never, and, dasyprocta, litigation, this,\n",
      "Nearest to see : is, canaris, akita, considers, cubes, blend, ah, centuries,\n",
      "Nearest to are : were, is, have, was, by, gland, be, asterism,\n",
      "Nearest to five : eight, four, six, seven, three, zero, nine, two,\n",
      "Nearest to often : now, mattingly, mats, dasyprocta, salinas, shab, a, also,\n",
      "Nearest to may : can, would, waza, eight, tragedians, dasyprocta, nine, to,\n",
      "Average loss at step 32000 : 5.87546185231\n",
      "Average loss at step 34000 : 5.8339735496\n",
      "Average loss at step 36000 : 5.73484470379\n",
      "Average loss at step 38000 : 5.28538503897\n",
      "Average loss at step 40000 : 5.47259349346\n",
      "Nearest to use : victoriae, dasyprocta, velar, vs, unborn, llama, opiate, reginae,\n",
      "Nearest to its : their, the, his, some, bed, harlan, a, metis,\n",
      "Nearest to seven : eight, five, six, four, nine, three, zero, one,\n",
      "Nearest to there : it, they, he, still, and, handler, she, not,\n",
      "Nearest to by : was, with, in, be, from, as, is, were,\n",
      "Nearest to to : would, imprint, for, nine, can, from, will, dasyprocta,\n",
      "Nearest to all : gland, subkey, abitibi, dasyprocta, journalists, many, two, agouti,\n",
      "Nearest to first : best, rangoon, agouti, brady, blow, hora, infections, defend,\n",
      "Nearest to history : shaman, saul, tickets, alvar, recitative, alpina, blaxploitation, russians,\n",
      "Nearest to at : in, on, agouti, with, dasyprocta, cl, from, and,\n",
      "Nearest to which : that, also, it, this, but, albury, never, and,\n",
      "Nearest to see : canaris, cubes, akita, considers, is, referenced, ah, blend,\n",
      "Nearest to are : were, is, have, was, gland, be, asterism, by,\n",
      "Nearest to five : four, six, seven, three, eight, zero, nine, two,\n",
      "Nearest to often : now, mats, also, mattingly, dasyprocta, it, generally, who,\n",
      "Nearest to may : can, would, waza, hydrothermal, dasyprocta, tragedians, must, three,\n",
      "Average loss at step 42000 : 5.30186149359\n",
      "Average loss at step 44000 : 5.29712139511\n",
      "Average loss at step 46000 : 5.2605512054\n",
      "Average loss at step 48000 : 5.02441073215\n",
      "Average loss at step 50000 : 5.16865433538\n",
      "Nearest to use : beeb, victoriae, dasyprocta, velar, perfective, vs, unborn, opiate,\n",
      "Nearest to its : their, his, the, harlan, bed, moravia, reginae, metis,\n",
      "Nearest to seven : eight, six, five, nine, four, three, zero, one,\n",
      "Nearest to there : it, they, he, still, she, dasyprocta, triad, handler,\n",
      "Nearest to by : was, be, with, from, as, were, in, for,\n",
      "Nearest to to : would, will, imprint, nine, bouldering, valentinians, albury, can,\n",
      "Nearest to all : abitibi, gland, subkey, dasyprocta, many, journalists, agouti, ontology,\n",
      "Nearest to first : best, brady, last, agouti, in, hora, backus, defend,\n",
      "Nearest to history : shaman, alvar, tickets, saul, recitative, dipyramid, russians, alphorn,\n",
      "Nearest to at : in, on, prism, agouti, with, and, dasyprocta, agni,\n",
      "Nearest to which : that, this, also, it, but, albury, one, never,\n",
      "Nearest to see : canaris, cubes, is, akita, referenced, asterism, maud, blend,\n",
      "Nearest to are : were, is, have, gland, be, was, do, including,\n",
      "Nearest to five : four, six, three, seven, eight, zero, nine, two,\n",
      "Nearest to often : now, also, mats, generally, which, jen, mattingly, naaman,\n",
      "Nearest to may : can, would, must, will, should, could, hydrothermal, waza,\n",
      "Average loss at step 52000 : 5.16755377603\n",
      "Average loss at step 54000 : 5.11729208422\n",
      "Average loss at step 56000 : 5.05119970107\n",
      "Average loss at step 58000 : 5.08750091422\n",
      "Average loss at step 60000 : 4.93481853378\n",
      "Nearest to use : beeb, victoriae, dasyprocta, marmoset, perfective, callithrix, velar, llama,\n",
      "Nearest to its : their, his, the, her, some, metis, ssbn, reginae,\n",
      "Nearest to seven : eight, six, five, nine, four, three, zero, two,\n",
      "Nearest to there : they, it, he, still, she, handler, not, triad,\n",
      "Nearest to by : was, be, as, with, were, under, naaman, wct,\n",
      "Nearest to to : would, imprint, nine, bouldering, dasyprocta, will, for, vs,\n",
      "Nearest to all : many, gland, abitibi, subkey, these, dasyprocta, some, agouti,\n",
      "Nearest to first : best, last, agouti, microsite, hora, brady, mcclellan, rangoon,\n",
      "Nearest to history : microcebus, michelob, wct, recitative, cebus, alpina, dasyprocta, tickets,\n",
      "Nearest to at : in, on, cebus, with, prism, under, agouti, callithrix,\n",
      "Nearest to which : that, this, also, it, but, callithrix, albury, one,\n",
      "Nearest to see : canaris, akita, cubes, asterism, ssbn, referenced, prism, tamarin,\n",
      "Nearest to are : were, is, have, be, gland, do, including, ssbn,\n",
      "Nearest to five : four, six, three, eight, seven, nine, zero, two,\n",
      "Nearest to often : now, also, generally, wct, mats, jen, which, dasyprocta,\n",
      "Nearest to may : can, would, must, will, could, should, hydrothermal, waza,\n",
      "Average loss at step 62000 : 4.78494637048\n",
      "Average loss at step 64000 : 4.79264241815\n",
      "Average loss at step 66000 : 4.97760552907\n",
      "Average loss at step 68000 : 4.91262453341\n",
      "Average loss at step 70000 : 4.75980962324\n",
      "Nearest to use : beeb, victoriae, dasyprocta, marmoset, callithrix, perfective, velar, fuerteventura,\n",
      "Nearest to its : their, his, the, her, ssbn, some, metis, agouti,\n",
      "Nearest to seven : six, eight, four, nine, three, five, zero, one,\n",
      "Nearest to there : they, it, still, he, we, she, often, handler,\n",
      "Nearest to by : was, be, in, as, were, from, with, wct,\n",
      "Nearest to to : would, can, will, imprint, for, vs, nine, bouldering,\n",
      "Nearest to all : many, some, gland, these, abitibi, subkey, dasyprocta, hands,\n",
      "Nearest to first : best, last, hora, second, defend, agouti, mcclellan, microsite,\n",
      "Nearest to history : microcebus, michelob, mitral, wct, saul, tickets, recitative, cebus,\n",
      "Nearest to at : in, on, prism, cebus, during, agouti, dasyprocta, wct,\n",
      "Nearest to which : that, this, also, but, it, callithrix, albury, beaches,\n",
      "Nearest to see : canaris, akita, cubes, ssbn, thaler, referenced, asterism, prism,\n",
      "Nearest to are : were, is, have, be, including, gland, do, sutra,\n",
      "Nearest to five : four, six, three, eight, seven, zero, nine, two,\n",
      "Nearest to often : now, generally, also, wct, sometimes, mats, jen, which,\n",
      "Nearest to may : can, would, will, must, could, should, hydrothermal, might,\n",
      "Average loss at step 72000 : 4.80357911384\n",
      "Average loss at step 74000 : 4.77398807603\n",
      "Average loss at step 76000 : 4.89073816597\n",
      "Average loss at step 78000 : 4.80883758235\n",
      "Average loss at step 80000 : 4.7976956389\n",
      "Nearest to use : beeb, victoriae, clodius, dasyprocta, marmoset, callithrix, crb, perfective,\n",
      "Nearest to its : their, his, the, her, ssbn, vec, reginae, agouti,\n",
      "Nearest to seven : six, eight, five, four, three, nine, zero, callithrix,\n",
      "Nearest to there : it, they, he, still, we, she, clodius, often,\n",
      "Nearest to by : was, cegep, be, in, from, with, vec, wct,\n",
      "Nearest to to : mitsuda, imprint, would, will, bouldering, wct, vs, dasyprocta,\n",
      "Nearest to all : many, some, these, gland, dasyprocta, subkey, abitibi, two,\n",
      "Nearest to first : last, best, second, hora, mcclellan, defend, next, agouti,\n",
      "Nearest to history : microcebus, michelob, wct, mitral, tickets, shaman, saul, alphorn,\n",
      "Nearest to at : in, on, cegep, during, prism, cebus, dasyprocta, wct,\n",
      "Nearest to which : that, this, also, it, but, callithrix, albury, cebus,\n",
      "Nearest to see : canaris, akita, cubes, ssbn, prism, thaler, asterism, lobbied,\n",
      "Nearest to are : were, is, have, including, do, be, gland, while,\n",
      "Nearest to five : six, four, seven, eight, three, nine, zero, two,\n",
      "Nearest to often : now, generally, also, sometimes, wct, usually, commonly, which,\n",
      "Nearest to may : can, would, will, could, must, should, might, crb,\n",
      "Average loss at step 82000 : 4.80440880048\n",
      "Average loss at step 84000 : 4.79721266794\n",
      "Average loss at step 86000 : 4.75776743543\n",
      "Average loss at step 88000 : 4.69816068482\n",
      "Average loss at step 90000 : 4.75949970603\n",
      "Nearest to use : beeb, victoriae, dasyprocta, marmoset, clodius, callithrix, crb, perfective,\n",
      "Nearest to its : their, his, the, her, ssbn, some, reginae, fath,\n",
      "Nearest to seven : six, eight, five, four, nine, three, zero, agouti,\n",
      "Nearest to there : they, it, he, still, she, we, often, clodius,\n",
      "Nearest to by : cegep, was, be, when, vec, as, through, wct,\n",
      "Nearest to to : would, imprint, mitsuda, will, nine, can, albury, wct,\n",
      "Nearest to all : many, some, these, gland, dasyprocta, peacocks, abitibi, several,\n",
      "Nearest to first : best, last, second, hora, agouti, next, microsite, mcclellan,\n",
      "Nearest to history : microcebus, michelob, wct, mitral, tickets, shaman, dipyramid, saul,\n",
      "Nearest to at : in, on, during, under, cegep, cebus, prism, wct,\n",
      "Nearest to which : that, this, also, but, it, callithrix, albury, both,\n",
      "Nearest to see : akita, canaris, yyyy, ssbn, cubes, but, asterism, thaler,\n",
      "Nearest to are : were, is, have, be, including, do, gland, while,\n",
      "Nearest to five : seven, six, four, eight, three, nine, zero, two,\n",
      "Nearest to often : now, generally, sometimes, also, usually, commonly, wct, mats,\n",
      "Nearest to may : can, would, will, could, must, should, might, crb,\n",
      "Average loss at step 92000 : 4.6982008096\n",
      "Average loss at step 94000 : 4.61954891503\n",
      "Average loss at step 96000 : 4.74444079077\n",
      "Average loss at step 98000 : 4.61056392497\n",
      "Average loss at step 100000 : 4.67929824364\n",
      "Nearest to use : beeb, victoriae, dasyprocta, callithrix, marmoset, clodius, crb, perfective,\n",
      "Nearest to its : their, his, the, her, ssbn, some, fath, reginae,\n",
      "Nearest to seven : six, eight, five, four, nine, three, zero, two,\n",
      "Nearest to there : they, it, he, still, we, she, however, often,\n",
      "Nearest to by : be, cegep, was, after, were, as, vec, when,\n",
      "Nearest to to : would, will, can, nine, imprint, wct, albury, bouldering,\n",
      "Nearest to all : many, some, these, several, dasyprocta, gland, peacocks, abitibi,\n",
      "Nearest to first : last, second, best, hora, next, original, mcclellan, microsite,\n",
      "Nearest to history : microcebus, wct, mitral, michelob, tickets, shaman, cebus, saul,\n",
      "Nearest to at : in, on, during, cegep, under, cebus, agni, charcot,\n",
      "Nearest to which : that, this, also, but, it, callithrix, these, one,\n",
      "Nearest to see : akita, cubes, canaris, yyyy, list, thaler, ssbn, but,\n",
      "Nearest to are : were, is, have, be, do, including, while, include,\n",
      "Nearest to five : four, seven, six, eight, three, zero, nine, two,\n",
      "Nearest to often : now, generally, sometimes, usually, commonly, also, wct, still,\n",
      "Nearest to may : can, would, will, could, must, should, might, cannot,\n"
     ]
    }
   ],
   "source": [
    "num_steps = 100001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    init.run()\n",
    "    print 'Initialized'\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(batch_size, num_skips, skip_window)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "            print 'Average loss at step {} : {}'.format(step, average_loss)\n",
    "            average_loss = 0\n",
    "\n",
    "        if step % 10000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in range(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log_str = 'Nearest to {} :'.format(valid_word)\n",
    "\n",
    "                for k in range(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log_str = '{} {},'.format(log_str, close_word)\n",
    "                print log_str\n",
    "        final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def plot_with_labels(low_dim_embs,labels,filename='tsne.png'):\n",
    "    assert low_dim_embs.shape[0]>=len(labels),'More labels then embeddings'\n",
    "    plt.figure(figsize=(18,18))\n",
    "    for i,label in enumerate(labels):\n",
    "        x,y=low_dim_embs[i,:]\n",
    "        plt.scatter(x,y)\n",
    "        plt.annotate(label,xy=(x,y),xytext=(5,2),textcoords='offset points',ha='right',va='bottom')\n",
    "    plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "tsne=TSNE(perplexity=30,n_components=2,init='pca',n_iter=5000)\n",
    "plot_only=100\n",
    "low_dim_embs=tsne.fit_transform(final_embeddings[:plot_only,:])\n",
    "labels=[reverse_dictionary[i] for i in range(plot_only)]\n",
    "plot_with_labels(low_dim_embs,labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
